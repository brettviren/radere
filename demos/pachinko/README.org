#+title: A toy differentiable simulation

* Overview

This toy is to play with a "differentiable simulation" where a model
is driven in the "forward" direction to transform parameters plus
input data into output data and with a second instance of the model
attempt to recover these parameter values through optimization.

This content is also in a [[file:pachinko.ipynb][ipython notebook]].

* Forward simulation

This toy is a painfully simple model of ionization electron readout
and reconstruction with a LAr TPC detector.

It is structured into a chain:

1) Production of ionization electrons at a point ("depos")
2) Transport ("drift") of the depos through an applied electric field to an linear array of electrodes
3) Binning ("collection") of the resulting distribution of ionization electrons into readout channels.

The remaining sections go through the code for each step and highlight
some of the (absurd) simplifications that are made.

** Depos

We generate ionization electrons at random points throughout a 2D
volume.

#+begin_src python :session pachinko :exports both :results output
  import pachinko
  nbatches = 10000
  nperevent = 10
  depos = pachinko.random_depos(nbatches, nperevent)
  
  def dump_deposet(depos):
      print(depos.shape)
      print("\tx\ty\tq\tsigma")
      for one in depos[:,:]:
          print("".join([f'{t:9.5}' for t in one.unbind()]))
  dump_deposet(depos[0])
  
#+end_src

#+RESULTS:
#+begin_example
torch.Size([10, 4])
	x	y	q	sigma
   400.21   10.482      1.0      0.0
   981.36  -29.389      1.0      0.0
   778.87  -8.6392      1.0      0.0
   740.75   9.1906      1.0      0.0
   325.02   33.845      1.0      0.0
   924.13  -18.358      1.0      0.0
   675.75  0.41654      1.0      0.0
   189.92  -19.848      1.0      0.0
   369.16 0.004828      1.0      0.0
   45.501  -41.773      1.0      0.0
#+end_example

Simplifications:

- Ionization electrons normally produced by a particle tracking
  simulation such as Geant4 where the $dE/dx$ of each step determines
  the number of electrons.  

- Normally, depos are generated in the 3D volume of the detector and
  their distributions ultimately projected into a 2D space associated
  with the array of electrodes.


** Drift

During transport, the electrons in each depo are subject to two
effects, each governed by one parameter that will be subject to
optimization later.

First is that as the group of electrons drift, some will reabsorb into
the LAr at an exponential rate governed by a "lifetime" parameter
(expressed here in units of distance).

Second is that the group undergoes transverse diffusion governed by a
 "DT" parameter resulting in a finite Gaussian extent.

#+begin_src python :session pachinko :exports both :results output
  import torch
  true_dt = 0.01
  true_lt = 3000.0
  drifter = pachinko.Drifter(true_dt, true_lt)
  print('Drifter       DT:', drifter.param_dt.item(), drifter.DT.item())
  print('Drifter lifetime:', drifter.param_lt.item(), drifter.lifetime.item())
  with torch.no_grad():
      drifted = drifter(depos[0])
  dump_deposet(drifted)
#+end_src

#+RESULTS:
#+begin_example
Drifter       DT: -4.595119953155518 0.009999998845160007
Drifter lifetime: -0.8472977876663208 3000.0
torch.Size([10, 4])
	x	y	q	sigma
      0.0   10.482  0.87511   2.8292
      0.0  -29.389    0.721   4.4303
      0.0  -8.6392  0.77134   3.9468
      0.0   9.1906  0.78121    3.849
      0.0   33.845  0.89732   2.5496
      0.0  -18.358  0.73488   4.2991
      0.0  0.41654  0.79832   3.6763
      0.0  -19.848  0.93866   1.9489
      0.0 0.004828  0.88422   2.7172
      0.0  -41.773  0.98495  0.95395
#+end_example

Simplifications:

- Fluctuations in the amount of reabsorption as well as in deviation
  from a Gaussian are normally included.

- In a 2D model, longitudinal diffusion along the drift direction is
  included.  It is ignored here as we fully integrate over this drift
  time.

** Collect

Each Gaussian distribution of drifted ionization electrons is directly
projected to the linear array of electrodes.  We integrate over all
such projections in order to determine the total number of electrons
that are "collected" by each readout channel.  

#+begin_src python :session pachinko :exports both :results output
  nwires = 21
  pitch = 5.0
  span = nwires * pitch
  half = 0.5*span
  bins = torch.linspace(-half, half, nwires+1)
  collector = pachinko.Collector(bins)
  with torch.no_grad():
      adcs = collector(drifted)
  print(adcs)
#+end_src

#+RESULTS:
: tensor([0.0000e+00, 1.3883e-01, 8.4959e-01, 1.1218e-01, 4.0928e-01, 2.7518e-01,
:         1.2676e+00, 3.7124e-01, 4.6427e-01, 4.4161e-01, 1.2543e+00, 5.1501e-01,
:         1.1659e+00, 2.2358e-01, 1.0854e-03, 1.9531e-04, 2.0428e-01, 6.7368e-01,
:         1.9165e-02, 7.7486e-07, 0.0000e+00])

Simplifications:

- The structure of ionization electron distribution along the drift
  direction is sampled by integrating the readout over short times
  (sampling at 2 MHz over a few ms).
- The drifting charge induces current in the electrodes via a detector
  response.  The resulting waveforms across electrodes are normally
  produced by a convolution of the ionization electron distribution
  with a fine-grained (1/10'th pitch) 2D model of this response.
- Further, the induced currents are converted to voltage, resulting in
  another response convolution and then truncated to integer value
  with a model of an ADC.
- The responses are then removed through a deconvolution using a
  kernel which is that of the detector+electronics response but
  averaged over one pitch.
- Finally, signal regions of interest are located on top of an
  unavoidably amplified low-frequency noise background and across a
  new baseline is defined locally across these regions and the result
  represents a tomographic measure of the original ionization electron
  distribution.


* Backward

The above shows how to generate an input set of depos, manually send
that set forward through the two "modules" (drifter and collector) to
produce one output histogram per depo set.  To test ability to
optimize the two parameters (DT and lifetime) we make two pairs:

- reality :: fixed ~DT=0.01~ and ~lifetime=3000.0~ and is given to the dataset as a transformation
- model :: initial ~DT=0.02~ and ~lifetime=2000.0~ which will be optimized so output matches "reality" dataset.

** Two models

To illustrate we make model as an ~nn.Sequential()~ with a new pair of
modules:

#+begin_src python :session pachinko :exports both :results output
  reality = pachinko.make_model(0.01, 3000.0, pitch, nwires)
  model = pachinko.make_model(0.02, 2000.0, pitch, nwires)
  drifter, collector = reality.children()
  with torch.no_grad():
      r_adcs = reality(depos[0])
      m_adcs = model(depos[0])
  print(r_adcs)
  print(m_adcs)  
#+end_src

#+RESULTS:
: tensor([0.0000e+00, 1.3883e-01, 8.4959e-01, 1.1218e-01, 4.0928e-01, 2.7518e-01,
:         1.2676e+00, 3.7124e-01, 4.6427e-01, 4.4161e-01, 1.2543e+00, 5.1501e-01,
:         1.1659e+00, 2.2358e-01, 1.0854e-03, 1.9531e-04, 2.0428e-01, 6.7368e-01,
:         1.9165e-02, 7.7486e-07, 0.0000e+00])
: tensor([1.3301e-05, 2.1920e-01, 7.7886e-01, 1.2749e-01, 2.6970e-01, 3.4254e-01,
:         1.0284e+00, 4.1951e-01, 3.6892e-01, 5.1284e-01, 9.5978e-01, 6.4981e-01,
:         8.5351e-01, 3.1374e-01, 1.5854e-02, 5.6526e-03, 2.4864e-01, 5.3138e-01,
:         6.4236e-02, 2.9314e-04, 0.0000e+00])


Next, we show for one depo set what the "reality" and the starting
"model" produces.

#+begin_src python :session pachinko :exports code :results file
  import matplotlib.pyplot as plt
  plt.clf()
  ax1 = plt.subplot(311)
  ax1.scatter(depos[0, :, 1], depos[0, :, 0])
  ax2 = plt.subplot(312, sharex=ax1)
  ax2.step(collector.binning[:-1], r_adcs)
  ax3 = plt.subplot(313, sharex=ax2, sharey=ax2)
  ax3.step(collector.binning[:-1], m_adcs)
  plt.tight_layout()
  fname = "figure1.png"
  plt.savefig(fname)
  fname
#+end_src

#+RESULTS:
[[file:figure1.png]]


Note, that "reality" should have less absorption (longer lifetime) and
less diffusion so should have higher, more pointed peaks than the
initial "model".


** Training

#+begin_src  python :session pachinko :exports both :results output
  nper = 10
  batch_size = 100
  nevent = batch_size * 1000
  
  pachinko.test_train(epochs=1, batch_size=batch_size,
                      learning_rate=0.1,
                      num_workers=4,
                      nevent=nevent, nper=nper)  
#+end_src

#+RESULTS:
#+begin_example
parameter: ('0.param_dt', Parameter containing:
tensor(-3.8918, requires_grad=True))
parameter: ('0.param_lt', Parameter containing:
tensor(-1.3863, requires_grad=True))
Epoch 1
-------------------------------
loss: 0.010555  [    0/100000] DT:0.019959788769483566 lt:2002.101806640625
loss: 0.009803  [ 1000/100000] DT:0.019567115232348442 lt:2022.5638427734375
loss: 0.009607  [ 2000/100000] DT:0.019188804551959038 lt:2042.6387939453125
loss: 0.008893  [ 3000/100000] DT:0.01882762275636196 lt:2062.300048828125
loss: 0.008149  [ 4000/100000] DT:0.01848597824573517 lt:2081.31494140625
loss: 0.007593  [ 5000/100000] DT:0.01816253736615181 lt:2099.883544921875
loss: 0.007323  [ 6000/100000] DT:0.01784868724644184 lt:2118.156494140625
loss: 0.007298  [ 7000/100000] DT:0.01755143329501152 lt:2135.604248046875
loss: 0.006564  [ 8000/100000] DT:0.017266839742660522 lt:2152.582275390625
loss: 0.006364  [ 9000/100000] DT:0.016988525167107582 lt:2169.4072265625
loss: 0.006211  [10000/100000] DT:0.016724154353141785 lt:2185.816650390625
loss: 0.005678  [11000/100000] DT:0.016469920054078102 lt:2201.849853515625
loss: 0.005652  [12000/100000] DT:0.016225645318627357 lt:2217.264404296875
loss: 0.004964  [13000/100000] DT:0.015992164611816406 lt:2232.45751953125
loss: 0.004751  [14000/100000] DT:0.01576717011630535 lt:2247.278076171875
loss: 0.004568  [15000/100000] DT:0.015550054609775543 lt:2261.867431640625
loss: 0.004388  [16000/100000] DT:0.01534689124673605 lt:2275.8642578125
loss: 0.004055  [17000/100000] DT:0.015150189399719238 lt:2289.69775390625
loss: 0.003557  [18000/100000] DT:0.014963733963668346 lt:2303.05517578125
loss: 0.003605  [19000/100000] DT:0.014779768884181976 lt:2316.11767578125
loss: 0.003466  [20000/100000] DT:0.014607706107199192 lt:2328.980224609375
loss: 0.003262  [21000/100000] DT:0.014439456164836884 lt:2341.2177734375
loss: 0.003040  [22000/100000] DT:0.014275933615863323 lt:2353.282470703125
loss: 0.002910  [23000/100000] DT:0.014120581559836864 lt:2365.2294921875
loss: 0.002693  [24000/100000] DT:0.013970942236483097 lt:2376.72021484375
loss: 0.002607  [25000/100000] DT:0.01383174117654562 lt:2387.7578125
loss: 0.002305  [26000/100000] DT:0.01369370799511671 lt:2398.705810546875
loss: 0.002470  [27000/100000] DT:0.013561805710196495 lt:2409.342041015625
loss: 0.002162  [28000/100000] DT:0.013434674590826035 lt:2419.859130859375
loss: 0.001996  [29000/100000] DT:0.013314682990312576 lt:2430.044189453125
loss: 0.002102  [30000/100000] DT:0.013198323547840118 lt:2439.929443359375
loss: 0.001696  [31000/100000] DT:0.013086441904306412 lt:2449.510009765625
loss: 0.001667  [32000/100000] DT:0.01297706551849842 lt:2459.053955078125
loss: 0.001615  [33000/100000] DT:0.01287383958697319 lt:2468.25390625
loss: 0.001634  [34000/100000] DT:0.012772493064403534 lt:2477.365234375
loss: 0.001420  [35000/100000] DT:0.01267730351537466 lt:2486.125244140625
loss: 0.001391  [36000/100000] DT:0.012582425028085709 lt:2494.7900390625
loss: 0.001275  [37000/100000] DT:0.012493458576500416 lt:2503.170654296875
loss: 0.001224  [38000/100000] DT:0.012406433001160622 lt:2511.351806640625
loss: 0.001211  [39000/100000] DT:0.012320824898779392 lt:2519.455322265625
loss: 0.001169  [40000/100000] DT:0.01224022638052702 lt:2527.187255859375
loss: 0.001110  [41000/100000] DT:0.01216039527207613 lt:2534.88330078125
loss: 0.001005  [42000/100000] DT:0.01208519283682108 lt:2542.478759765625
loss: 0.000992  [43000/100000] DT:0.012011226266622543 lt:2549.734130859375
loss: 0.000949  [44000/100000] DT:0.011939387768507004 lt:2556.964111328125
loss: 0.000805  [45000/100000] DT:0.01187179982662201 lt:2563.910400390625
loss: 0.000807  [46000/100000] DT:0.01180646475404501 lt:2570.776611328125
loss: 0.000750  [47000/100000] DT:0.011744833551347256 lt:2577.297607421875
loss: 0.000775  [48000/100000] DT:0.011683275923132896 lt:2583.808837890625
loss: 0.000731  [49000/100000] DT:0.011621885932981968 lt:2590.196044921875
loss: 0.000664  [50000/100000] DT:0.011565767228603363 lt:2596.27099609375
loss: 0.000615  [51000/100000] DT:0.011510693468153477 lt:2602.38623046875
loss: 0.000603  [52000/100000] DT:0.011457249522209167 lt:2608.3525390625
loss: 0.000585  [53000/100000] DT:0.011406105943024158 lt:2614.20751953125
loss: 0.000539  [54000/100000] DT:0.011356760747730732 lt:2619.827392578125
loss: 0.000509  [55000/100000] DT:0.011309449560940266 lt:2625.318115234375
loss: 0.000496  [56000/100000] DT:0.01126151718199253 lt:2630.751953125
loss: 0.000471  [57000/100000] DT:0.011215428821742535 lt:2636.142578125
loss: 0.000440  [58000/100000] DT:0.01117180846631527 lt:2641.32861328125
loss: 0.000419  [59000/100000] DT:0.011129667051136494 lt:2646.46728515625
loss: 0.000403  [60000/100000] DT:0.011088603176176548 lt:2651.4169921875
loss: 0.000383  [61000/100000] DT:0.011049685068428516 lt:2656.28515625
loss: 0.000359  [62000/100000] DT:0.011011742055416107 lt:2661.039306640625
loss: 0.000333  [63000/100000] DT:0.010975362733006477 lt:2665.7392578125
loss: 0.000316  [64000/100000] DT:0.010939056053757668 lt:2670.295166015625
loss: 0.000299  [65000/100000] DT:0.010904806666076183 lt:2674.769775390625
loss: 0.000297  [66000/100000] DT:0.010870805941522121 lt:2679.189208984375
loss: 0.000289  [67000/100000] DT:0.01083817332983017 lt:2683.533935546875
loss: 0.000277  [68000/100000] DT:0.010806585662066936 lt:2687.766357421875
loss: 0.000266  [69000/100000] DT:0.010775145143270493 lt:2691.9287109375
loss: 0.000232  [70000/100000] DT:0.010745984502136707 lt:2695.901123046875
loss: 0.000245  [71000/100000] DT:0.010716578923165798 lt:2699.918701171875
loss: 0.000223  [72000/100000] DT:0.010688199661672115 lt:2703.855712890625
loss: 0.000213  [73000/100000] DT:0.010660926811397076 lt:2707.656005859375
loss: 0.000201  [74000/100000] DT:0.010635136626660824 lt:2711.38037109375
loss: 0.000189  [75000/100000] DT:0.010610380209982395 lt:2714.996337890625
loss: 0.000187  [76000/100000] DT:0.010586025193333626 lt:2718.605712890625
loss: 0.000177  [77000/100000] DT:0.010562711395323277 lt:2722.12158203125
loss: 0.000169  [78000/100000] DT:0.010539772920310497 lt:2725.5048828125
loss: 0.000159  [79000/100000] DT:0.01051734946668148 lt:2728.907958984375
loss: 0.000150  [80000/100000] DT:0.010496031492948532 lt:2732.2236328125
loss: 0.000151  [81000/100000] DT:0.01047550980001688 lt:2735.441650390625
loss: 0.000139  [82000/100000] DT:0.010455352254211903 lt:2738.620849609375
loss: 0.000130  [83000/100000] DT:0.010435380972921848 lt:2741.806884765625
loss: 0.000132  [84000/100000] DT:0.010416386649012566 lt:2744.90576171875
loss: 0.000128  [85000/100000] DT:0.010397760197520256 lt:2747.94091796875
loss: 0.000117  [86000/100000] DT:0.010379856452345848 lt:2750.897705078125
loss: 0.000115  [87000/100000] DT:0.01036269310861826 lt:2753.77490234375
loss: 0.000105  [88000/100000] DT:0.01034602615982294 lt:2756.6201171875
loss: 0.000104  [89000/100000] DT:0.010330146178603172 lt:2759.384521484375
loss: 0.000107  [90000/100000] DT:0.010314343497157097 lt:2762.144775390625
loss: 0.000089  [91000/100000] DT:0.010299532674252987 lt:2764.806640625
loss: 0.000088  [92000/100000] DT:0.010284936986863613 lt:2767.440673828125
loss: 0.000084  [93000/100000] DT:0.010270836763083935 lt:2770.025146484375
loss: 0.000086  [94000/100000] DT:0.010257089510560036 lt:2772.629150390625
loss: 0.000083  [95000/100000] DT:0.01024386752396822 lt:2775.1357421875
loss: 0.000082  [96000/100000] DT:0.010231136344373226 lt:2777.58056640625
loss: 0.000075  [97000/100000] DT:0.010218999348580837 lt:2779.975830078125
loss: 0.000068  [98000/100000] DT:0.010206824168562889 lt:2782.337158203125
loss: 0.000073  [99000/100000] DT:0.010194950737059116 lt:2784.68701171875
parameter: ('0.param_dt', Parameter containing:
tensor(-4.5766, requires_grad=True)) tensor(0.0011)
parameter: ('0.param_lt', Parameter containing:
tensor(-0.9510, requires_grad=True)) tensor(-0.0011)
Done!
#+end_example

** 30'th epoch

#+begin_example
Epoch 30
-------------------------------
loss: 0.000001  [    0/100000] DT:0.01037519983947277 lt:2984.672607421875
loss: 0.000001  [ 1000/100000] DT:0.01037481240928173 lt:2984.686767578125
loss: 0.000001  [ 2000/100000] DT:0.010374430567026138 lt:2984.7001953125
loss: 0.000001  [ 3000/100000] DT:0.010374054312705994 lt:2984.714111328125
loss: 0.000001  [ 4000/100000] DT:0.010373666882514954 lt:2984.726318359375
loss: 0.000001  [ 5000/100000] DT:0.010373280383646488 lt:2984.7412109375
loss: 0.000001  [ 6000/100000] DT:0.010372903198003769 lt:2984.75537109375
loss: 0.000001  [ 7000/100000] DT:0.010372522287070751 lt:2984.770751953125
loss: 0.000001  [ 8000/100000] DT:0.010372149758040905 lt:2984.787841796875
loss: 0.000001  [ 9000/100000] DT:0.01037176325917244 lt:2984.80224609375
loss: 0.000001  [10000/100000] DT:0.01037138607352972 lt:2984.816650390625
loss: 0.000001  [11000/100000] DT:0.010371014475822449 lt:2984.831298828125
loss: 0.000001  [12000/100000] DT:0.01037063729017973 lt:2984.84716796875
loss: 0.000001  [13000/100000] DT:0.010370261035859585 lt:2984.863525390625
loss: 0.000001  [14000/100000] DT:0.010369888506829739 lt:2984.880615234375
loss: 0.000001  [15000/100000] DT:0.01036952156573534 lt:2984.897705078125
loss: 0.000001  [16000/100000] DT:0.010369149968028069 lt:2984.91357421875
loss: 0.000001  [17000/100000] DT:0.010368768125772476 lt:2984.92822265625
loss: 0.000001  [18000/100000] DT:0.010368386283516884 lt:2984.9404296875
loss: 0.000001  [19000/100000] DT:0.010368000715970993 lt:2984.953125
loss: 0.000001  [20000/100000] DT:0.0103676225990057 lt:2984.967041015625
loss: 0.000001  [21000/100000] DT:0.010367241688072681 lt:2984.98095703125
loss: 0.000001  [22000/100000] DT:0.010366865433752537 lt:2984.995361328125
loss: 0.000001  [23000/100000] DT:0.010366497561335564 lt:2985.011962890625
loss: 0.000001  [24000/100000] DT:0.010366121307015419 lt:2985.02685546875
loss: 0.000001  [25000/100000] DT:0.010365739464759827 lt:2985.040771484375
loss: 0.000001  [26000/100000] DT:0.010365343652665615 lt:2985.05517578125
loss: 0.000001  [27000/100000] DT:0.01036496739834547 lt:2985.071044921875
loss: 0.000001  [28000/100000] DT:0.010364605113863945 lt:2985.086669921875
loss: 0.000001  [29000/100000] DT:0.01036424282938242 lt:2985.102294921875
loss: 0.000001  [30000/100000] DT:0.010363872162997723 lt:2985.1171875
loss: 0.000001  [31000/100000] DT:0.010363499633967876 lt:2985.1328125
loss: 0.000001  [32000/100000] DT:0.01036312896758318 lt:2985.1474609375
loss: 0.000001  [33000/100000] DT:0.010362747125327587 lt:2985.16162109375
loss: 0.000001  [34000/100000] DT:0.010362384840846062 lt:2985.176025390625
loss: 0.000001  [35000/100000] DT:0.010362023487687111 lt:2985.191162109375
loss: 0.000001  [36000/100000] DT:0.010361656546592712 lt:2985.20556640625
loss: 0.000001  [37000/100000] DT:0.01036128494888544 lt:2985.22021484375
loss: 0.000001  [38000/100000] DT:0.01036092359572649 lt:2985.23583984375
loss: 0.000001  [39000/100000] DT:0.010360551066696644 lt:2985.25244140625
loss: 0.000001  [40000/100000] DT:0.01036018505692482 lt:2985.26708984375
loss: 0.000001  [41000/100000] DT:0.010359818115830421 lt:2985.283203125
loss: 0.000001  [42000/100000] DT:0.010359451174736023 lt:2985.29736328125
loss: 0.000001  [43000/100000] DT:0.010359089821577072 lt:2985.31103515625
loss: 0.000001  [44000/100000] DT:0.01035870797932148 lt:2985.32421875
loss: 0.000001  [45000/100000] DT:0.010358355939388275 lt:2985.339111328125
loss: 0.000001  [46000/100000] DT:0.010357999242842197 lt:2985.354248046875
loss: 0.000001  [47000/100000] DT:0.010357627645134926 lt:2985.369384765625
loss: 0.000001  [48000/100000] DT:0.010357270948588848 lt:2985.384765625
loss: 0.000001  [49000/100000] DT:0.01035691425204277 lt:2985.401123046875
loss: 0.000001  [50000/100000] DT:0.010356547310948372 lt:2985.416748046875
loss: 0.000001  [51000/100000] DT:0.010356190614402294 lt:2985.430908203125
loss: 0.000001  [52000/100000] DT:0.010355829261243343 lt:2985.445068359375
loss: 0.000001  [53000/100000] DT:0.010355458594858646 lt:2985.456787109375
loss: 0.000001  [54000/100000] DT:0.010355097241699696 lt:2985.471435546875
loss: 0.000001  [55000/100000] DT:0.010354739613831043 lt:2985.485595703125
loss: 0.000001  [56000/100000] DT:0.010354382917284966 lt:2985.50048828125
loss: 0.000001  [57000/100000] DT:0.01035403087735176 lt:2985.516845703125
loss: 0.000001  [58000/100000] DT:0.010353674180805683 lt:2985.531005859375
loss: 0.000001  [59000/100000] DT:0.010353303514420986 lt:2985.544921875
loss: 0.000001  [60000/100000] DT:0.010352951474487782 lt:2985.558837890625
loss: 0.000001  [61000/100000] DT:0.010352585464715958 lt:2985.572509765625
loss: 0.000001  [62000/100000] DT:0.010352213867008686 lt:2985.585693359375
loss: 0.000001  [63000/100000] DT:0.010351852513849735 lt:2985.599853515625
loss: 0.000001  [64000/100000] DT:0.010351491160690784 lt:2985.61572265625
loss: 0.000001  [65000/100000] DT:0.010351124219596386 lt:2985.629638671875
loss: 0.000001  [66000/100000] DT:0.010350758209824562 lt:2985.642822265625
loss: 0.000001  [67000/100000] DT:0.010350396856665611 lt:2985.65673828125
loss: 0.000001  [68000/100000] DT:0.010350030846893787 lt:2985.67138671875
loss: 0.000001  [69000/100000] DT:0.010349677875638008 lt:2985.687744140625
loss: 0.000001  [70000/100000] DT:0.010349331423640251 lt:2985.70263671875
loss: 0.000001  [71000/100000] DT:0.010348975658416748 lt:2985.717041015625
loss: 0.000001  [72000/100000] DT:0.010348609648644924 lt:2985.731689453125
loss: 0.000001  [73000/100000] DT:0.01034825760871172 lt:2985.749755859375
loss: 0.000001  [74000/100000] DT:0.010347911156713963 lt:2985.762939453125
loss: 0.000001  [75000/100000] DT:0.010347560048103333 lt:2985.77685546875
loss: 0.000001  [76000/100000] DT:0.010347217321395874 lt:2985.789794921875
loss: 0.000001  [77000/100000] DT:0.010346866212785244 lt:2985.804443359375
loss: 0.000001  [78000/100000] DT:0.01034651417285204 lt:2985.818603515625
loss: 0.000001  [79000/100000] DT:0.010346158407628536 lt:2985.83203125
loss: 0.000001  [80000/100000] DT:0.010345806367695332 lt:2985.847412109375
loss: 0.000001  [81000/100000] DT:0.010345449671149254 lt:2985.860595703125
loss: 0.000001  [82000/100000] DT:0.010345098562538624 lt:2985.874755859375
loss: 0.000001  [83000/100000] DT:0.010344752110540867 lt:2985.890869140625
loss: 0.000001  [84000/100000] DT:0.010344395413994789 lt:2985.90576171875
loss: 0.000001  [85000/100000] DT:0.010344044305384159 lt:2985.918212890625
loss: 0.000001  [86000/100000] DT:0.010343692265450954 lt:2985.931640625
loss: 0.000001  [87000/100000] DT:0.010343331843614578 lt:2985.94384765625
loss: 0.000001  [88000/100000] DT:0.010342979803681374 lt:2985.957763671875
loss: 0.000001  [89000/100000] DT:0.01034262403845787 lt:2985.972412109375
loss: 0.000001  [90000/100000] DT:0.010342267341911793 lt:2985.985595703125
loss: 0.000001  [91000/100000] DT:0.010341916233301163 lt:2986.00048828125
loss: 0.000001  [92000/100000] DT:0.010341574437916279 lt:2986.01513671875
loss: 0.000001  [93000/100000] DT:0.010341217741370201 lt:2986.02978515625
loss: 0.000001  [94000/100000] DT:0.010340866632759571 lt:2986.04150390625
loss: 0.000001  [95000/100000] DT:0.010340530425310135 lt:2986.0546875
loss: 0.000001  [96000/100000] DT:0.010340183041989803 lt:2986.06640625
loss: 0.000001  [97000/100000] DT:0.01033984124660492 lt:2986.080322265625
loss: 0.000001  [98000/100000] DT:0.010339510627090931 lt:2986.094970703125
loss: 0.000001  [99000/100000] DT:0.01033917348831892 lt:2986.109619140625
#+end_example
